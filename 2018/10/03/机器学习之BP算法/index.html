
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Fitz的博客">
    <title>机器学习之BP算法 - Fitz的博客</title>
    <meta name="author" content="Fitz">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Fitz","sameAs":["https://github.com/Fitz1003Miao","mailto:fitz1003m@gmail.com"],"image":"cover.jpg"},"articleBody":"神经网络 Back Propagation 算法相关公式的推导以及具体代码实现\n\n\n从一个具体的、简单的网络模型来一步一步推导BP公式\n网络结构介绍最简单的神经网络，包括输入层、隐藏层、输出层三层，其中，我们规定\n\n输入层 3 个神经元\n隐藏层 5 个神经元\n输出层 3个神经元\n\n其中输入层、隐藏层各包含一个 bias 单元，\n最终的神经网络结构如下图所示：\n\n网络参数输入样本\\vec x = [x_1, x_2, x_3]第一层网络参数W^{(1)} = \\begin{bmatrix} W_{(x_1, 4)} & W_{(x_1, 5)} & W_{(x_1, 6)} & W_{(x_1, 7)} & W_{(x_1, 8)} \\\\ W_{(x_2, 4)} & W_{(x_2, 5)} & W_{(x_2, 6)} & W_{(x_2, 7)} & W_{(x_2,  8)} \\\\ W_{(x_3, 4)} & W_{(x_3, 5)}  & W_{(x_3, 6)}  & W_{(x_3, 7)}  & W_{(x_3, 8)} \\end{bmatrix}b^{(1)} = \\begin{bmatrix} b_1 &  b_2 & b_3 & b_4  & b_5 \\end{bmatrix}第二层网络参数W^{(2)} = \\begin{bmatrix}W_{(4, 9)} & W_{(4, 10)} & W_{(4, 11)}\\\\ W_{(5, 9)} & W_{(5, 10)} & W_{(5, 11)}\\\\ W_{(6, 9)} & W_{(6, 10)} & W_{(6, 11)}\\\\ W_{(7, 9)} & W_{(7, 10)} & W_{(7, 11)}\\\\ W_{(8, 9)} & W_{(8, 10)} & W_{(8, 11)}\\end{bmatrix}b^{(2)} = \\begin{bmatrix}b_6 & b_7 & b_8\\end{bmatrix}激励函数为了增强神经网络的非线性表达能力— 即可以表示出更加复杂的模型，引入激励函数，在这里采用的是『sigmoid』函数：\ng(x) = \\frac{1}{1 + e^{-x}}前向传播输入层\n输入 $\\vec x$\n向隐藏层传输的结果为 $ \\vec x$\n\n隐藏层\n输入 $ \\vec x$\n输出为 $z_1 = \\vec x * W^{(1)} + b^{(1)}$\n最后向隐藏层传输的结果为 $ n_1 = g(z_1)$\n\n输出层\n输入 $n_1$\n输出为$z_2 = n_1 * W^{(2)} + b^{(2)}$\n最后向输出层传输的结果为$n_2 = g(z_2)$\n\n最终结果为 $n_2$\n反向传播什么是反向传播通俗来说就是根据 最终的结果以及 costFunction 来计算出当前模型的loss，再来计算每个权值在当前 loss 所占的比例，进而更新每个权值，使 loss更小。在这里这个比例我们采用『梯度』来表示。\n通过观察神经网络的结构，可以发现，一个节点的输出会影响后面层中所有节点的结果，所以一个权值会影响下一层的某个节点输出，进而影响下一层之后的所有层结果。当我们来计算当前权值在 loss 中所占的比例，需要将它分摊到后面各层之中，所以为了计算简便(不重复计算)以及条理清楚，我们从最后一层开始计算，然后再计算倒数第二层…..如此即为反向传播。\n公式推导在这里，引入几个符号：\n\nL 为当前模型的 loss\n$z_i$ 为第 i 层的输出\n$n_i$ 为第 i 层向下一层传输的结果\n\n那么由上一节『前向传播』 可得:\nz_{i + 1} = n_i * W^{(i + 1)} + b^{(i+1)}要进行梯度更新，就要计算loss对于权值的梯度，即计算 $\\frac{\\partial {L}}{\\partial W^{(i)}}$ 、$\\frac{\\partial L}{\\partial b^{(i)}}$根据链式求导法则可得：\n\\frac{\\partial L}{\\partial W^{(i)}} = \\frac{\\partial L}{\\partial z_i} * \\frac{\\partial z_i}{\\partial W^{(i)}}\\frac{\\partial L}{\\partial b^{(i)}} = \\frac{\\partial L}{\\partial z_i} * \\frac{\\partial z_i}{\\partial b^{(i)}}所以转而计算 $\\frac{\\partial L}{\\partial z_i}$、$\\frac{\\partial z_i}{\\partial W^{(i)}}$、$\\frac{\\partial z_i}{\\partial b^{(i)}}$三项\n\\frac{\\partial z_i}{\\partial W^{(i)}} = n_{i - 1}^{T}\\frac{\\partial z_i}{\\partial b^{(i)}} = E其中E为单位矩阵\n而 $\\frac{\\partial L}{\\partial z_i}$ 的计算有些麻烦，令 $\\delta ^{(i)} = \\frac{\\partial L}{\\partial z_i}$\n当我们计算 $\\delta ^{(i)}$ 时，首先需要计算出$\\delta ^{(i + 1)}$,然后反向计算出$\\delta ^{(i)}$, 运用链式求导法则可得：\n\\delta ^{(i)} = \\frac{\\partial L}{\\partial z_{i}} = \\frac{\\partial L}{\\partial z_{i+1}} * \\frac{\\partial z_{i+1}}{\\partial n_i} * \\frac{\\partial n_i}{\\partial z_i}= \\delta ^{(i + 1)} * \\frac{\\partial z_{i+1}}{\\partial n_i} * \\frac{\\partial n_i}{\\partial z_i}\\frac{\\partial z_{i+1}}{\\partial n_i} = (W^{(i+1)})^{T}\\frac{\\partial n_i}{\\partial z_i} = g(z_i) * (1 - g(z_i))所以\n\\delta ^{(i)} = \\delta ^{(i + 1)} * (W^{(i+1)})^{T} * g(z_i) * (1 - g(z_i))最后可得：\n\\frac{\\partial L}{\\partial W^{(i)}} = \\delta ^{(i)} * n_{i - 1}^{T}\\frac{\\partial L}{\\partial b^{(i)}} = \\delta ^{(i)}具体模型为了简单起见，我们用 『均方误差』 来表示模型的loss，即：\nL = \\frac{1}{2}\\sum{(y - \\hat y)^2}其中 $y = n_2$\n要进行反向传播，需要计算$\\frac{\\partial {L}}{\\partial W^{(1)}}$ 、$\\frac{\\partial L}{\\partial b^{(1)}}$、$\\frac{\\partial {L}}{\\partial W^{(2)}}$ 、$\\frac{\\partial L}{\\partial b^{(2)}}$\n\\delta ^{(2)} = \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial n_2} * \\frac{\\partial n_2}{\\partial z_2} = \\frac{\\partial L}{\\partial y} * \\frac{\\partial y}{\\partial z_2}\\frac{\\partial L}{\\partial y} = y - \\hat y\\frac{\\partial y}{\\partial z_2} = g(z_2) * (1 - g(z_x))\\delta ^{(2)} = (y - \\hat y) * g(z_2) * (1 - g(z_x))\\frac{\\partial {L}}{\\partial W^{(2)}} = \\delta ^{(2)} * n_{1}^{T}\\frac{\\partial L}{\\partial b^{(2)}} =  \\delta ^{(2)}\\delta ^{(1)} =  \\delta ^{(2)} * (W^{(2)})^{T} * g(z_1) * (1 - g(z_1))\\frac{\\partial {L}}{\\partial W^{(1)}} =  \\delta ^{(1)} * n_{0}^{T}此时$n_0 = \\vec x$\n代码实现细节上面推导的公式都是在单样本情况下，而在多样本情况下会出现维度问题，比如矩阵进行运算时候维度要对齐，还有就是在求$\\frac{\\partial L}{\\partial b^{(i)}} $的时候，由于$\\delta^{(i)}$ 是大小是样本量相关的，所以我们求得$\\delta ^{(i)}$后要对其进行求和操作，可以看成每个样本对bias单元的影响之和。\n梯度检验在用代码实现之后，如何检查loss Function 和 gradient是否对应，这个时候就要用梯度的定义了。梯度即为导数，而根据导数定义\n{f(x)}' = \\frac{\\partial f}{\\partial x} = \\frac{f(x + \\varepsilon ) - f(x - \\varepsilon )}{2 * \\varepsilon }其中 $ \\varepsilon$ 足够小\n可以得到$\\frac{\\partial L}{\\partial W_{(x_1, 4)}}$等等所有的值，再进行一一比较，只要在误差允许范围内，表明计算正确，loss function 和 gradient 是对应的。\n代码传送门\n","dateCreated":"2018-10-03T19:46:18+08:00","dateModified":"2018-10-07T18:11:07+08:00","datePublished":"2018-10-03T19:46:18+08:00","description":"神经网络 Back Propagation 算法相关公式的推导以及具体代码实现","headline":"机器学习之BP算法","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2018/10/03/机器学习之BP算法/"},"publisher":{"@type":"Organization","name":"Fitz","sameAs":["https://github.com/Fitz1003Miao","mailto:fitz1003m@gmail.com"],"image":"cover.jpg","logo":{"@type":"ImageObject","url":"cover.jpg"}},"url":"http://yoursite.com/2018/10/03/机器学习之BP算法/","keywords":"Machine Learning"}</script>
    <meta name="description" content="神经网络 Back Propagation 算法相关公式的推导以及具体代码实现">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="blog">
<meta property="og:title" content="机器学习之BP算法">
<meta property="og:url" content="http://yoursite.com/2018/10/03/机器学习之BP算法/index.html">
<meta property="og:site_name" content="Fitz的博客">
<meta property="og:description" content="神经网络 Back Propagation 算法相关公式的推导以及具体代码实现">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/img/机器学习之BP算法/1.png">
<meta property="og:updated_time" content="2018-10-07T10:11:07.874Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习之BP算法">
<meta name="twitter:description" content="神经网络 Back Propagation 算法相关公式的推导以及具体代码实现">
<meta name="twitter:image" content="http://yoursite.com/img/机器学习之BP算法/1.png">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com../../../../assets/images/cover.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="../../../../assets/css/style-du2xmrqdqrl2ollgeiw050kpl6l4nbyz7bumjuurjgsxyopifvukebxc9lqe.min.css">
    <!--STYLES END-->
    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="3">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="../../../../ ">Fitz的博客</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="../../../../assets/images/cover.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="3">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="../../../../#about">
                    <img class="sidebar-profile-picture" src="../../../../assets/images/cover.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Fitz</h4>
                
                    <h5 class="sidebar-profile-bio"><p>SDU Master</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../../../../ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../../../../all-categories"
                            
                            title="categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../../../../all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../../../../all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/Fitz1003Miao" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:fitz1003m@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fa fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="3"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            机器学习之BP算法
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-10-03T19:46:18+08:00">
	
		    Oct 03, 2018
    	
    </time>
    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>神经网络 Back Propagation 算法相关公式的推导以及具体代码实现</p>
<a id="more"></a>
<h1 id="table-of-contents">文章目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络结构介绍"><span class="toc-text">网络结构介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络参数"><span class="toc-text">网络参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#输入样本"><span class="toc-text">输入样本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第一层网络参数"><span class="toc-text">第一层网络参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二层网络参数"><span class="toc-text">第二层网络参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激励函数"><span class="toc-text">激励函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#前向传播"><span class="toc-text">前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#输入层"><span class="toc-text">输入层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#隐藏层"><span class="toc-text">隐藏层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输出层"><span class="toc-text">输出层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播"><span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是反向传播"><span class="toc-text">什么是反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#公式推导"><span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#具体模型"><span class="toc-text">具体模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现细节"><span class="toc-text">代码实现细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度检验"><span class="toc-text">梯度检验</span></a></li></ol></li></ol>
<p>从一个具体的、简单的网络模型来一步一步推导BP公式</p>
<h2 id="网络结构介绍"><a href="#网络结构介绍" class="headerlink" title="网络结构介绍"></a>网络结构介绍</h2><p>最简单的神经网络，包括输入层、隐藏层、输出层三层，其中，我们规定</p>
<ul>
<li>输入层 3 个神经元</li>
<li>隐藏层 5 个神经元</li>
<li>输出层 3个神经元</li>
</ul>
<p>其中输入层、隐藏层各包含一个 bias 单元，</p>
<p>最终的神经网络结构如下图所示：</p>
<p><img src="/img/机器学习之BP算法/1.png" alt="网络结构"></p>
<h2 id="网络参数"><a href="#网络参数" class="headerlink" title="网络参数"></a>网络参数</h2><h3 id="输入样本"><a href="#输入样本" class="headerlink" title="输入样本"></a>输入样本</h3><script type="math/tex; mode=display">\vec x = [x_1, x_2, x_3]</script><h3 id="第一层网络参数"><a href="#第一层网络参数" class="headerlink" title="第一层网络参数"></a>第一层网络参数</h3><script type="math/tex; mode=display">W^{(1)} = \begin{bmatrix} W_{(x_1, 4)} & W_{(x_1, 5)} & W_{(x_1, 6)} & W_{(x_1, 7)} & W_{(x_1, 8)} \\ W_{(x_2, 4)} & W_{(x_2, 5)} & W_{(x_2, 6)} & W_{(x_2, 7)} & W_{(x_2,  8)} \\ W_{(x_3, 4)} & W_{(x_3, 5)}  & W_{(x_3, 6)}  & W_{(x_3, 7)}  & W_{(x_3, 8)} \end{bmatrix}</script><script type="math/tex; mode=display">b^{(1)} = \begin{bmatrix} b_1 &  b_2 & b_3 & b_4  & b_5 \end{bmatrix}</script><h3 id="第二层网络参数"><a href="#第二层网络参数" class="headerlink" title="第二层网络参数"></a>第二层网络参数</h3><script type="math/tex; mode=display">W^{(2)} = \begin{bmatrix}W_{(4, 9)} & W_{(4, 10)} & W_{(4, 11)}\\ W_{(5, 9)} & W_{(5, 10)} & W_{(5, 11)}\\ W_{(6, 9)} & W_{(6, 10)} & W_{(6, 11)}\\ W_{(7, 9)} & W_{(7, 10)} & W_{(7, 11)}\\ W_{(8, 9)} & W_{(8, 10)} & W_{(8, 11)}\end{bmatrix}</script><script type="math/tex; mode=display">b^{(2)} = \begin{bmatrix}b_6 & b_7 & b_8\end{bmatrix}</script><h3 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h3><p>为了增强神经网络的非线性表达能力— 即可以表示出更加复杂的模型，引入激励函数，在这里采用的是『sigmoid』函数：</p>
<script type="math/tex; mode=display">g(x) = \frac{1}{1 + e^{-x}}</script><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><ul>
<li>输入 $\vec x$</li>
<li>向隐藏层传输的结果为 $ \vec x$</li>
</ul>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><ul>
<li>输入 $ \vec x$</li>
<li>输出为 $z_1 = \vec x * W^{(1)} + b^{(1)}$</li>
<li>最后向隐藏层传输的结果为 $ n_1 = g(z_1)$</li>
</ul>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><ul>
<li>输入 $n_1$</li>
<li>输出为$z_2 = n_1 * W^{(2)} + b^{(2)}$</li>
<li>最后向输出层传输的结果为$n_2 = g(z_2)$</li>
</ul>
<p>最终结果为 $n_2$</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="什么是反向传播"><a href="#什么是反向传播" class="headerlink" title="什么是反向传播"></a>什么是反向传播</h3><p>通俗来说就是根据 最终的结果以及 costFunction 来计算出当前模型的loss，再来计算每个权值在当前 loss 所占的比例，进而更新每个权值，使 loss更小。在这里这个比例我们采用『梯度』来表示。</p>
<p>通过观察神经网络的结构，可以发现，一个节点的输出会影响后面层中所有节点的结果，所以一个权值会影响下一层的某个节点输出，进而影响下一层之后的所有层结果。当我们来计算当前权值在 loss 中所占的比例，需要将它分摊到后面各层之中，所以为了计算简便(不重复计算)以及条理清楚，我们从最后一层开始计算，然后再计算倒数第二层…..如此即为反向传播。</p>
<h3 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h3><p>在这里，引入几个符号：</p>
<ul>
<li>L 为当前模型的 loss</li>
<li>$z_i$ 为第 i 层的输出</li>
<li>$n_i$ 为第 i 层向下一层传输的结果</li>
</ul>
<p>那么由上一节『前向传播』 可得:</p>
<script type="math/tex; mode=display">z_{i + 1} = n_i * W^{(i + 1)} + b^{(i+1)}</script><p>要进行梯度更新，就要计算loss对于权值的梯度，即计算 $\frac{\partial {L}}{\partial W^{(i)}}$ 、$\frac{\partial L}{\partial b^{(i)}}$根据链式求导法则可得：</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W^{(i)}} = \frac{\partial L}{\partial z_i} * \frac{\partial z_i}{\partial W^{(i)}}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b^{(i)}} = \frac{\partial L}{\partial z_i} * \frac{\partial z_i}{\partial b^{(i)}}</script><p>所以转而计算 $\frac{\partial L}{\partial z_i}$、$\frac{\partial z_i}{\partial W^{(i)}}$、$\frac{\partial z_i}{\partial b^{(i)}}$三项</p>
<script type="math/tex; mode=display">\frac{\partial z_i}{\partial W^{(i)}} = n_{i - 1}^{T}</script><script type="math/tex; mode=display">\frac{\partial z_i}{\partial b^{(i)}} = E</script><p>其中E为单位矩阵</p>
<p>而 $\frac{\partial L}{\partial z_i}$ 的计算有些麻烦，令 $\delta ^{(i)} = \frac{\partial L}{\partial z_i}$</p>
<p>当我们计算 $\delta ^{(i)}$ 时，首先需要计算出$\delta ^{(i + 1)}$,然后反向计算出$\delta ^{(i)}$, 运用链式求导法则可得：</p>
<script type="math/tex; mode=display">\delta ^{(i)} = \frac{\partial L}{\partial z_{i}} = \frac{\partial L}{\partial z_{i+1}} * \frac{\partial z_{i+1}}{\partial n_i} * \frac{\partial n_i}{\partial z_i}= \delta ^{(i + 1)} * \frac{\partial z_{i+1}}{\partial n_i} * \frac{\partial n_i}{\partial z_i}</script><script type="math/tex; mode=display">\frac{\partial z_{i+1}}{\partial n_i} = (W^{(i+1)})^{T}</script><script type="math/tex; mode=display">\frac{\partial n_i}{\partial z_i} = g(z_i) * (1 - g(z_i))</script><p>所以</p>
<script type="math/tex; mode=display">\delta ^{(i)} = \delta ^{(i + 1)} * (W^{(i+1)})^{T} * g(z_i) * (1 - g(z_i))</script><p>最后可得：</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W^{(i)}} = \delta ^{(i)} * n_{i - 1}^{T}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b^{(i)}} = \delta ^{(i)}</script><h3 id="具体模型"><a href="#具体模型" class="headerlink" title="具体模型"></a>具体模型</h3><p>为了简单起见，我们用 『均方误差』 来表示模型的loss，即：</p>
<script type="math/tex; mode=display">L = \frac{1}{2}\sum{(y - \hat y)^2}</script><p>其中 $y = n_2$</p>
<p>要进行反向传播，需要计算$\frac{\partial {L}}{\partial W^{(1)}}$ 、$\frac{\partial L}{\partial b^{(1)}}$、$\frac{\partial {L}}{\partial W^{(2)}}$ 、$\frac{\partial L}{\partial b^{(2)}}$</p>
<script type="math/tex; mode=display">\delta ^{(2)} = \frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial n_2} * \frac{\partial n_2}{\partial z_2} = \frac{\partial L}{\partial y} * \frac{\partial y}{\partial z_2}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial y} = y - \hat y</script><script type="math/tex; mode=display">\frac{\partial y}{\partial z_2} = g(z_2) * (1 - g(z_x))</script><script type="math/tex; mode=display">\delta ^{(2)} = (y - \hat y) * g(z_2) * (1 - g(z_x))</script><script type="math/tex; mode=display">\frac{\partial {L}}{\partial W^{(2)}} = \delta ^{(2)} * n_{1}^{T}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b^{(2)}} =  \delta ^{(2)}</script><script type="math/tex; mode=display">\delta ^{(1)} =  \delta ^{(2)} * (W^{(2)})^{T} * g(z_1) * (1 - g(z_1))</script><script type="math/tex; mode=display">\frac{\partial {L}}{\partial W^{(1)}} =  \delta ^{(1)} * n_{0}^{T}</script><p>此时$n_0 = \vec x$</p>
<h2 id="代码实现细节"><a href="#代码实现细节" class="headerlink" title="代码实现细节"></a>代码实现细节</h2><p>上面推导的公式都是在单样本情况下，而在多样本情况下会出现维度问题，比如矩阵进行运算时候维度要对齐，还有就是在求$\frac{\partial L}{\partial b^{(i)}} $的时候，由于$\delta^{(i)}$ 是大小是样本量相关的，所以我们求得$\delta ^{(i)}$后要对其进行求和操作，可以看成每个样本对bias单元的影响之和。</p>
<h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>在用代码实现之后，如何检查loss Function 和 gradient是否对应，这个时候就要用梯度的定义了。梯度即为导数，而根据导数定义</p>
<script type="math/tex; mode=display">{f(x)}' = \frac{\partial f}{\partial x} = \frac{f(x + \varepsilon ) - f(x - \varepsilon )}{2 * \varepsilon }</script><p>其中 $ \varepsilon$ 足够小</p>
<p>可以得到$\frac{\partial L}{\partial W_{(x_1, 4)}}$等等所有的值，再进行一一比较，只要在误差允许范围内，表明计算正确，loss function 和 gradient 是对应的。</p>
<p><a href="https://github.com/Fitz1003Miao/NGhomework/blob/master/ex4/nn.py" target="_blank" rel="noopener">代码传送门</a></p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="../../../../tags/Machine-Learning/">Machine Learning</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="../../05/Bias-Variance-trade-off/" data-tooltip="Bias-Variance trade-off" aria-label="PREVIOUS: Bias-Variance trade-off">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="文章目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Fitz. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="3">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="../../05/Bias-Variance-trade-off/" data-tooltip="Bias-Variance trade-off" aria-label="PREVIOUS: Bias-Variance trade-off">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="文章目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="3">
    <i id="btn-close-shareoptions" class="fa fa-times"></i>
    <ul class="share-options">
        
    </ul>
</div>

            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="../../../../assets/images/cover.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Fitz</h4>
        
            <div id="about-card-bio"><p>SDU Master</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Student</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                QingDao, CHN
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('../../../../assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="../../../../assets/js/script-vufjrm3fmbuttogo1hxuu0w9w0sesk5iyysjuguc2hdhufot9szxg8twijry.min.js"></script>
<!--SCRIPTS END-->

    



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --!>
</body>
</html>
